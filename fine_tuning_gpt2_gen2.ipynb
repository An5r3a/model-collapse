{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV9qasdhULt_",
        "outputId": "2ae932e5-f61d-4158-bf64-6fdaaa188059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n",
            "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.17.0 textstat-0.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "91RQpQ9li_8U"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split, RandomSampler, SequentialSampler\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ksc1T34jRff"
      },
      "outputs": [],
      "source": [
        "def load_prompts(file_path):\n",
        "    \"\"\"\n",
        "    Reads a text file where harmful prompts start with \"-\" and may be separated by \"\\n\" within a line.\n",
        "    \"\"\"\n",
        "    prompts = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            s = f\"<|startoftext|>{line} <|endoftext|>\"\n",
        "            prompts.append(s)\n",
        "    return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKvwwGf6jWId"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for i in [1,2,3,4,5,6,7,8,9]:\n",
        "  file_path = f'/content/drive/MyDrive/Colab Notebooks/data_gen8/results_{i}.txt'\n",
        "  data += load_prompts(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CShEWrxxkHaq",
        "outputId": "21cfe6e3-8145-41bf-8e22-72bfc7cd5d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|startoftext|> \n",
            " <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "print(data[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W74YlTcky76"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2-medium\"\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name,\n",
        "                                              bos_token='<|startoftext|>',\n",
        "                                              eos_token='<|endoftext|>',\n",
        "                                              unk_token='<|unknown|>',\n",
        "                                              pad_token='<|pad|>'\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBYSkV0QktLX",
        "outputId": "a9caaa02-671a-4ad0-f775-0587efe23d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: tensor([50257,   220,   198,   220, 50256, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
            "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]) attn_masks: tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "max_length = 180\n",
        "\n",
        "# standard PyTorch approach of loading data in using a Dataset class.\n",
        "class HarmfulPromptsDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for dt in data:\n",
        "            encodings = tokenizer.encode_plus(dt,\n",
        "                                              truncation=True,\n",
        "                                              padding='max_length',\n",
        "                                              max_length=max_length,\n",
        "                                              return_tensors='pt'  # return a PyTorch tensor\n",
        "                                             )\n",
        "            self.input_ids.append(torch.squeeze(encodings['input_ids'],0))\n",
        "            self.attn_masks.append(torch.squeeze(encodings['attention_mask'],0))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "#dataset = HarmfulPromptsDataset(data[:1000], tokenizer)\n",
        "dataset = HarmfulPromptsDataset(data, tokenizer)\n",
        "print(f\"input_ids: {dataset[0][0]} attn_masks: {dataset[0][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x2p2Jtkk3jQ"
      },
      "outputs": [],
      "source": [
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create the DataLoaders for our training and validation datasets.\n",
        "# Get training samples in random order.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# Get valiation samples sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_fw_xTBHkK5L"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"gpt2-medium\"\n",
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/model_gen6/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcMSyAxzkmql"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n",
        "warmup_steps = 1e2\n",
        "# to prevent any division by zero in the implementation\n",
        "epsilon = 1e-8\n",
        "optim = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs  # [no batches] x [no epochs]\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optim,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AQ0wImGlFVY"
      },
      "outputs": [],
      "source": [
        "def infer(prompt):\n",
        "    input = f\"<|startoftext|>Prompt: {prompt.strip()}. <|endoftext|>\"\n",
        "    input = tokenizer(input, return_tensors=\"pt\")\n",
        "    input_ids      = input[\"input_ids\"]\n",
        "    attention_mask = input[\"attention_mask\"]\n",
        "\n",
        "    output = model.generate(input_ids.to(device),\n",
        "                            attention_mask=attention_mask.to(device),\n",
        "                            max_new_tokens=max_length,\n",
        "                            do_sample = True, top_k = 50, top_p = 0.95, temperature = 1)\n",
        "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw84Co1OlVl_",
        "outputId": "8f469f2a-97a9-4395-b795-6355af108ae4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n"
          ]
        }
      ],
      "source": [
        "for epoch_i in range(0, epochs):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels    = batch[0].to(device)\n",
        "        b_masks     = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids = b_input_ids, labels = b_labels,\n",
        "                         attention_mask = b_masks, token_type_ids = None )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            model.eval()\n",
        "            input_sequence = \"<|startoftext|>\"\n",
        "            inputs = torch.tensor(tokenizer.encode(input_sequence)).unsqueeze(0)\n",
        "\n",
        "            output = model.generate(inputs.to(device),\n",
        "                                    do_sample=True,\n",
        "                                            top_k=50,\n",
        "                                            max_length = 300,\n",
        "                                            top_p=0.95,\n",
        "                                            num_return_sequences=1)\n",
        "\n",
        "\n",
        "            output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "            print(output_text)\n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5YsyL8glcvi"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "# Evaluate data for one epoch\n",
        "for batch in validation_dataloader:\n",
        "  b_input_ids = batch[0].to(device)\n",
        "  b_labels    = batch[0].to(device)\n",
        "  b_masks     = batch[1].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs  = model(input_ids = b_input_ids, labels = b_labels, attention_mask = b_masks)\n",
        "    loss = outputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoBX-7J3lgEO",
        "outputId": "925dd979-92eb-48d9-abfa-6890811552be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/model_gen7/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/model_gen7/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/model_gen7/vocab.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/model_gen7/merges.txt',\n",
              " '/content/drive/MyDrive/Colab Notebooks/model_gen7/added_tokens.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/model_gen7/tokenizer.json')"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_gen_save_path = '/content/drive/MyDrive/Colab Notebooks/model_gen7/'\n",
        "model.save_pretrained(model_gen_save_path)\n",
        "tokenizer.save_pretrained(model_gen_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRpVsqWX-kkp"
      },
      "outputs": [],
      "source": [
        "def generate_sequences(top_k = 50, top_p=0.85, temperature = 0.7):\n",
        "  input_sequence = \"<|startoftext|>\"\n",
        "  inputs = torch.tensor(tokenizer.encode(input_sequence)).unsqueeze(0)\n",
        "\n",
        "  output = model.generate(inputs.to(device),\n",
        "                          do_sample=True,\n",
        "                                  top_k=top_k,\n",
        "                                  max_length = 300,\n",
        "                                  top_p=top_p,\n",
        "                                  temperature = temperature,\n",
        "                                  num_return_sequences=100)\n",
        "\n",
        "\n",
        "  output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  print(output_text)\n",
        "  return output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
