{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-medium\"\n",
    "model_gen = \"model_gen2\"\n",
    "data_gen = \"data_gen2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(f'/content/drive/MyDrive/Colab Notebooks/{model_gen}/')\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(f'/content/drive/MyDrive/Colab Notebooks/{model_gen}/')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentences = set()\n",
    "unique_words = set()\n",
    "\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "\n",
    "  file_path = f'/content/drive/MyDrive/Colab Notebooks/{data_gen}/results_{i}.txt'\n",
    "\n",
    "  with open(file_path, \"r\") as file:\n",
    "      for line in file:\n",
    "          sentence = line.replace(\"Prompt: \", \"\").strip()\n",
    "          if len(sentence)>0:\n",
    "            unique_sentences.add(sentence)\n",
    "            \n",
    "            words = sentence.split()\n",
    "            unique_words.update(words)\n",
    "\n",
    "print(f\"Number of unique sentences: {len(unique_sentences)}\")\n",
    "print(f\"Number of unique words: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "\n",
    "    file_path = f'/content/drive/MyDrive/Colab Notebooks/{data_gen}/results_{i}.txt'\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            sentence = line.replace(\"Prompt: \", \"\").strip()\n",
    "            \n",
    "            # Add the sentence to the set of unique sentences\n",
    "            total_sentences.append(sentence)\n",
    "\n",
    "words = [word.lower() for sentence in total_sentences for word in sentence.split()]\n",
    "lexical_diversity = len(set(words)) / len(words)\n",
    "print(f\"Lexical Diversity {data_gen}: {lexical_diversity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def compute_overall_readability(sentences):\n",
    "    total_flesch_reading_ease = 0\n",
    "    total_flesch_kincaid_grade = 0\n",
    "    total_gunning_fog_index = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        total_flesch_reading_ease += textstat.flesch_reading_ease(sentence)\n",
    "        total_flesch_kincaid_grade += textstat.flesch_kincaid_grade(sentence)\n",
    "        total_gunning_fog_index += textstat.gunning_fog(sentence)\n",
    "\n",
    "    num_sentences = len(sentences)\n",
    "    overall_flesch_reading_ease = total_flesch_reading_ease / num_sentences\n",
    "    overall_flesch_kincaid_grade = total_flesch_kincaid_grade / num_sentences\n",
    "    overall_gunning_fog_index = total_gunning_fog_index / num_sentences\n",
    "\n",
    "    return {\n",
    "        \"Overall Flesch Reading Ease\": overall_flesch_reading_ease,\n",
    "        \"Overall Flesch-Kincaid Grade\": overall_flesch_kincaid_grade,\n",
    "        \"Overall Gunning Fog Index\": overall_gunning_fog_index\n",
    "    }\n",
    "\n",
    "total_sentences = []\n",
    "for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "\n",
    "    file_path = f'/content/drive/MyDrive/Colab Notebooks/{data_gen}/results_{i}.txt'\n",
    "\n",
    "    # Process the text file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            sentence = line.replace(\"Prompt: \", \"\").strip()\n",
    "            \n",
    "            total_sentences.append(sentence)\n",
    "\n",
    "\n",
    "# Compute overall readability\n",
    "overall_scores = compute_overall_readability(total_sentences)\n",
    "print(f\"Iteration {data_gen}: {overall_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for data in ['data_gen2']:\n",
    "  total_sentences = []\n",
    "  for i in [0,1,2,3,4,5,6,7,8,9]:\n",
    "\n",
    "    file_path = f'/content/drive/MyDrive/Colab Notebooks/{data}/results_{i}.txt'\n",
    "\n",
    "    # Process the text file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            sentence = line.replace(\"Prompt: \", \"\").strip()\n",
    "            \n",
    "            # Add the sentence to the set of unique sentences\n",
    "            total_sentences.append(sentence)\n",
    "  polarity = []\n",
    "  for sentence in total_sentences[:455]:\n",
    "    if len(sentence)> 0:\n",
    "      blob = TextBlob(sentence)\n",
    "      polarity.append(blob.sentiment.polarity)\n",
    "\n",
    "  mean_polarity = sum(polarity) / len(polarity)\n",
    "  print(f'{data} polarity: {mean_polarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
